{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pycaret.classification as pyc\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "import ast\n",
    "import pydash\n",
    "import numpy as np\n",
    "\n",
    "import sys\n",
    "sys.path.append('../../../scripts/')\n",
    "import PreparaDatasetV3 as v3\n",
    "import CreationFuncs as cf\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET = 'Back_Home'\n",
    "ODDS = 'Odds_H'\n",
    "SPREAD = 1\n",
    "TYPE_BET = 'back'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cf.setup_model_params(target = TARGET, odds = ODDS, type_bet = TYPE_BET, spread = SPREAD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Carregando o dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../../../data/entire_season.csv')\n",
    "data = v3.prepara_df(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepara_dataset(df):\n",
    "    df = df.query(\"1.3 <= Odds_H <= 2\")\n",
    "    df = df.query(\"League == 'Eua Nba'\")\n",
    "\n",
    "    # Limpa df\n",
    "    df.replace([np.inf, -np.inf], 0, inplace=True)\n",
    "    # df.dropna(subset=['Avg_CG_H', 'Avg_CG_A', TARGET], inplace=True)\n",
    "\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data.copy()\n",
    "df = prepara_dataset(df)\n",
    "df['Date'] = pd.to_datetime(df['Date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3241 jogos no dataset\n",
      "Win rate: 60.63% - Odd relativa: 1.65\n",
      "Odd real: 1.59\n"
     ]
    }
   ],
   "source": [
    "# DIA: 18/01/2025\n",
    "df.query('Date < \"2025-01-18\"', inplace=True)\n",
    "print(f'{df.shape[0]} jogos no dataset')\n",
    "print(f'Win rate: {df[TARGET].mean() * 100:.2f}% - Odd relativa: {1/df[TARGET].mean():.2f}')\n",
    "print(f'Odd real: {df[ODDS].mean():.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Criar os blocos de n jogos (se não for usar clusterização)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# blocks of n rows\n",
    "dfs = cf.create_batches(df, 800)\n",
    "treinamento = pd.concat([dfs[0], dfs[1]])\n",
    "entire_val = pd.concat(dfs[2:], ignore_index=True)\n",
    "val = dfs[2]\n",
    "print(f'{len(dfs)} blocos')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Criação do modelo de cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['Over_Line', 'HA_Line', 'p_H', 'p_A', 'H_A', 'A_H', 'H_O','A_O','Euc_Dist_MO', 'Angle_MO', 'Euc_Dist_OV',\n",
    " 'Angle_OV', 'DifAbs_HomeAway', 'Angle_HomeAway', 'DifPer_HomeAway', 'Media_Ptos_H', 'Media_Ptos_A', 'DesvPad_Ptos_H', 'DesvPad_Ptos_A',\n",
    " 'CV_Ptos_H', 'CV_Ptos_A', ODDS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycaret.clustering import *\n",
    "\n",
    "stp = setup(data = df, ignore_features = [x for x in df.columns.to_list() if x not in features], verbose = False, session_id = 2024, memory = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_model = create_model('kmeans', 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustered = assign_model(c_model)\n",
    "clustered = pd.merge(df[[TARGET, 'Date', 'Home', 'Away', 'Home_Pts', 'Away_Pts']], clustered, left_index=True, right_index=True)\n",
    "\n",
    "# Separa jogos aleatoriamente em treinamento e validação\n",
    "treinamento, entire_val = train_test_split(clustered, train_size=0.40, random_state=2024)\n",
    "\n",
    "# Cria blocos de jogos\n",
    "dfs = cf.create_batches(entire_val, 500)\n",
    "print(f'{len(dfs)} dataframes\\n')\n",
    "\n",
    "print(f'Treino: {treinamento.shape[0]}\\n')\n",
    "\n",
    "c0 = treinamento[treinamento['Cluster'] == 'Cluster 0']\n",
    "c1 = treinamento[treinamento['Cluster'] == 'Cluster 1']\n",
    "c2 = treinamento[treinamento['Cluster'] == 'Cluster 2']\n",
    "c3 = treinamento[treinamento['Cluster'] == 'Cluster 3']\n",
    "c4 = treinamento[treinamento['Cluster'] == 'Cluster 4']\n",
    "c5 = treinamento[treinamento['Cluster'] == 'Cluster 5']\n",
    "c6 = treinamento[treinamento['Cluster'] == 'Cluster 6']\n",
    "c7 = treinamento[treinamento['Cluster'] == 'Cluster 7']\n",
    "\n",
    "print(f'Data normal - Média WR: {treinamento[TARGET].mean() * 100:.2f}%')\n",
    "print(f'Cluster 0 - Média WR: {c0[TARGET].mean() * 100:.2f}% ({c0.shape[0]} jogos)')\n",
    "print(f'Cluster 1 - Média WR: {c1[TARGET].mean() * 100:.2f}% ({c1.shape[0]} jogos)')\n",
    "print(f'Cluster 2 - Média WR: {c2[TARGET].mean() * 100:.2f}% ({c2.shape[0]} jogos)')\n",
    "print(f'Cluster 3 - Média WR: {c3[TARGET].mean() * 100:.2f}% ({c3.shape[0]} jogos)')\n",
    "print(f'Cluster 4 - Média WR: {c4[TARGET].mean() * 100:.2f}% ({c4.shape[0]} jogos)')\n",
    "print(f'Cluster 5 - Média WR: {c5[TARGET].mean() * 100:.2f}% ({c5.shape[0]} jogos)')\n",
    "print(f'Cluster 6 - Média WR: {c6[TARGET].mean() * 100:.2f}% ({c6.shape[0]} jogos)')\n",
    "print(f'Cluster 7 - Média WR: {c7[TARGET].mean() * 100:.2f}% ({c7.shape[0]} jogos)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analisa resultados de profit nos clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# c_profit = clustered.copy()\n",
    "# c_profit.query(\"Cluster == 'Cluster 3'\", inplace=True)\n",
    "# c_profit.loc[(c_profit[TARGET] == 0), 'Profit'] = 0.94\n",
    "# c_profit.loc[(c_profit[TARGET] == 1), 'Profit'] = - c_profit[ODDS] - 1\n",
    "# c_profit['Acumulado'] = c_profit['Profit'].cumsum()\n",
    "# c_profit_value = c_profit['Acumulado'].iloc[-1]\n",
    "\n",
    "# print(f'Profit: {c_profit_value:.2f}')\n",
    "# print(f'ROI: {c_profit_value / len(c_profit):.2f}')\n",
    "\n",
    "# c_profit[[ODDS]].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seleciona o cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHOSEN_CLUSTER = 'Cluster 1'\n",
    "\n",
    "treinamento.query('Cluster == @CHOSEN_CLUSTER or Cluster == \"Cluster 3\" or Cluster == \"Cluster 4\" or Cluster == \"Cluster 5\"', inplace=True)\n",
    "entire_val.query('Cluster == @CHOSEN_CLUSTER or Cluster == \"Cluster 3\" or Cluster == \"Cluster 4\" or Cluster == \"Cluster 5\"', inplace=True)\n",
    "\n",
    "for df in dfs:\n",
    "    df.query('Cluster == @CHOSEN_CLUSTER or Cluster == \"Cluster 3\" or Cluster == \"Cluster 4\" or Cluster == \"Cluster 5\"', inplace=True)\n",
    "\n",
    "val = dfs[0].copy()\n",
    "\n",
    "print(f'Treino: {treinamento.shape[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(c_model, 'cluster')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seleção de variáveis para o modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['FT_Odds_H', 'FT_Odds_D', 'FT_Odds_A', 'Odds_O25', 'Odds_U25', 'BTTS_Yes', 'BTTS_No', 'CV_Odds', 'H_D',\n",
    " 'H_A', 'D_H', 'D_A', 'A_H', 'A_D', 'O_D', 'O_S', 'H_O', 'A_O', 'Odds_Sum', 'HO_D', 'AO_D', 'ADJ_H', 'ADJ_A', 'Euc_Dist_MO',\n",
    " 'Angle_MO', 'Euc_Dist_OV', 'Angle_OV', 'DifAbs_HomeAway', 'DifAbs_HomeDraw', 'DifAbs_DrawAway', 'Angle_HomeAway',\n",
    " 'Angle_HomeDraw', 'Angle_DrawAway', 'DifPer_HomeAway', 'DifPer_HomeDraw', 'DifPer_DrawAway', ODDS]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variáveis escolhidas aleatóriamente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "colunas = features\n",
    "model_name = 'nb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "melhor_PL = -10000\n",
    "\n",
    "for i in range(30):\n",
    "    feats = cf.random_variables(var_list=colunas, min_vars=5, max_vars=15)\n",
    "    feats.extend([ODDS])\n",
    "\n",
    "    metrics = cf.create_model_sample(data = treinamento, fts = feats, seed=300, model_algorithm = model_name, fix_imbalance=True)\n",
    "\n",
    "    print(f'Rodada {i+1}')\n",
    "\n",
    "    if metrics['pl'] > melhor_PL:\n",
    "        melhor_PL = metrics['pl']\n",
    "        print(f\"**Novo melhor**\")\n",
    "        print(f\"Precision: {metrics['prec_model']:.4f} | STD: {metrics['std_model']:.2f}\")\n",
    "        print(f\"PL: {metrics['pl']:.2f} | ROI: {100*metrics['roi']:.2f}% | EM: {metrics['ev']:.2f} | Prec.: {metrics['wr']:.2f} | Entradas: {metrics['porc_ent']:.2f}\")\n",
    "        print(f\"features = {feats}\\n\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Redução de variáveis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "fits = ['FT_Odds_D', 'H_D', 'Odds', 'D_H', 'Angle_HomeAway', 'DifAbs_DrawAway', 'ADJ_A', 'Euc_Dist_MO', 'BTTS_Yes', 'DifAbs_HomeAway', 'H_A', 'BTTS_No', 'DifAbs_HomeDraw', 'Odds']\n",
    "fits = ['FT_Odds_D', 'H_D', 'Odds', 'D_H', 'DifAbs_DrawAway', 'ADJ_A', 'Euc_Dist_MO', 'BTTS_Yes', 'DifAbs_HomeAway', 'H_A', 'BTTS_No', 'DifAbs_HomeDraw', 'Odds']\n",
    "fits = ['FT_Odds_D', 'H_D', 'Odds', 'D_H', 'DifAbs_DrawAway', 'ADJ_A', 'BTTS_Yes', 'H_A', 'BTTS_No', 'DifAbs_HomeDraw', 'Odds']\n",
    "fits = ['FT_Odds_D', 'Odds', 'D_H', 'DifAbs_DrawAway', 'ADJ_A', 'BTTS_Yes', 'H_A', 'BTTS_No', 'DifAbs_HomeDraw', 'Odds']\n",
    "fits = ['FT_Odds_D', 'Odds', 'D_H', 'DifAbs_DrawAway', 'BTTS_Yes', 'H_A', 'BTTS_No', 'DifAbs_HomeDraw', 'Odds']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = cf.create_model_sample(data = treinamento, fts = fits, seed=300, model_algorithm = model_name, fix_imbalance=True)\n",
    "\n",
    "print('---------------------- BASE ----------------------')\n",
    "print(f'Precision: {metrics[\"prec_model\"]:.4f} | Std: {metrics[\"std_model\"]:.2f}')\n",
    "print(f'PL: {metrics[\"pl\"]:.2f} | ROI: {100*metrics[\"roi\"]:.2f}% | EM: {metrics[\"ev\"]:.2f} | Prec.: {metrics[\"wr\"]:.2f} | Entradas: {metrics[\"porc_ent\"]:.2f}')\n",
    "print(f'fits = {fits}\\n')\n",
    "print('--------------------------------------------------')\n",
    "melhor_PL = metrics[\"pl\"]\n",
    "\n",
    "for i in range(len(fits) - 1):\n",
    "    feat = fits[i]\n",
    "    fits.pop(i)\n",
    "    print(f'Removido: {feat}')\n",
    "\n",
    "    metrics = cf.create_model_sample(data = treinamento, fts = fits, seed=300, model_algorithm = model_name, fix_imbalance=True)\n",
    "\n",
    "    if metrics[\"pl\"] >= melhor_PL:\n",
    "        print('*** NOVO MELHOR ***')\n",
    "        melhor_PL = metrics[\"pl\"]\n",
    "    print(f'Precision: {metrics[\"prec_model\"]:.4f} | Std: {metrics[\"std_model\"]:.2f}')\n",
    "    print(f'PL: {metrics[\"pl\"]:.2f} | ROI: {100*metrics[\"roi\"]:.2f}% | EM: {metrics[\"ev\"]:.2f} | Prec.: {metrics[\"wr\"]:.2f} | Entradas: {metrics[\"porc_ent\"]:.2f}')\n",
    "    print(f'fits = {fits}\\n')\n",
    "    fits.insert(i, feat)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross validation manual para teste de seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['FT_Odds_H', 'FT_Odds_D', 'FT_Odds_A', 'Odds_O25', 'Odds_U25', 'BTTS_Yes', 'BTTS_No', 'CV_Odds', 'H_D',\n",
    " 'H_A', 'D_H', 'D_A', 'A_H', 'A_D', 'O_D', 'O_S', 'H_O', 'A_O', 'Odds_Sum', 'HO_D', 'AO_D', 'ADJ_H', 'ADJ_A', 'Euc_Dist_MO',\n",
    " 'Angle_MO', 'Euc_Dist_OV', 'Angle_OV', 'DifAbs_HomeAway', 'DifAbs_HomeDraw', 'DifAbs_DrawAway', 'Angle_HomeAway',\n",
    " 'Angle_HomeDraw', 'Angle_DrawAway', 'DifPer_HomeAway', 'DifPer_HomeDraw', 'DifPer_DrawAway', 'Media_Ptos_H', 'Media_Ptos_A',\n",
    " 'DesvPad_Ptos_H', 'DesvPad_Ptos_A', 'CV_Ptos_H', 'CV_Ptos_A', 'DifPtos_H', 'Media_GM_H', 'Media_GM_A', 'DesvPad_GM_H', 'DesvPad_GM_A',\n",
    " 'CV_GM_H', 'CV_GM_A', 'DifGM_H', 'Media_GS_H', 'Media_GS_A', 'DesvPad_GS_H', 'DesvPad_GS_A', 'CV_GS_H', 'CV_GS_A', 'DifGS_H',\n",
    " 'Media_SG_H', 'Media_SG_A', 'DesvPad_SG_H', 'DesvPad_SG_A', 'CV_SG_H', 'CV_SG_A', 'DifSG_H', 'Media_SG_H_01', 'Media_SG_A_01', 'DesvPad_SG_H_01',\n",
    " 'DesvPad_SG_A_01', 'CV_SG_H_01', 'CV_SG_A_01', 'Media_SG_H_02', 'Media_SG_A_02', 'DesvPad_SG_H_02', 'DesvPad_SG_A_02', 'CV_SG_H_02',\n",
    " 'CV_SG_A_02', 'Media_CG_H_02', 'Media_CG_A_02', 'DesvPad_CG_H_02', 'DesvPad_CG_A_02', 'CV_CG_H_02', 'CV_CG_A_02', 'Dif_CG_02', 'MediaOddsHome',\n",
    " 'MediaOddsAway', 'OMQB', 'xJoneHome2', 'xJoneAway2', 'Media_0x0_Home', 'Media_0x0_Away', 'Media_Goals_Home', 'Media_Goals_Away', 'Media_0G_Home',\n",
    " 'Media_0G_Away', ODDS]\n",
    "\n",
    "fits = ['FT_Odds_D', 'Odds', 'D_H', 'DifAbs_DrawAway', 'BTTS_Yes', 'H_A', 'BTTS_No', 'DifAbs_HomeDraw', 'Odds']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'nb'\n",
    "\n",
    "seeds = random.sample(range(0, 9999), 10)\n",
    "\n",
    "for i in range(len(seeds)):\n",
    "        metrics = cf.create_model_sample(data = treinamento, fts = features, seed=seeds[i], model_algorithm = model_name, fix_imbalance = True)\n",
    "\n",
    "        previsoes = metrics['dataframe']\n",
    "        pl = metrics['pl']\n",
    "        roi = metrics['roi']\n",
    "        oddback = metrics['oddback']\n",
    "        ev = metrics['ev']\n",
    "        wr = metrics['wr']\n",
    "        porc_ent = metrics['porc_ent']\n",
    "\n",
    "        print(f'Fold {i+1} (seed {seeds[i]})')\n",
    "        print(f'PL: {pl:.2f} | ROI: {100*roi:.2f}% | Odd média: {oddback:.2f} | EM: {ev:.2f} | Prec.: {wr:.2f} | Entradas: {porc_ent:.2f}')\n",
    "        cf.plot_mini_chart(previsoes)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2235\n",
    "7741"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Criação do modelo de classificação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [ 'Over_Line', 'HA_Line', 'p_H', 'p_A', 'H_A', 'A_H', 'H_O','A_O','Euc_Dist_MO', 'Angle_MO', 'Euc_Dist_OV',\n",
    " 'Angle_OV', 'DifAbs_HomeAway', 'Angle_HomeAway', 'DifPer_HomeAway', 'Media_Ptos_H', 'Media_Ptos_A', 'DesvPad_Ptos_H', 'DesvPad_Ptos_A',\n",
    " 'CV_Ptos_H', 'CV_Ptos_A', 'DifPtos_H', 'Media_GM_H', 'Media_GM_A', 'DesvPad_GM_H', 'DesvPad_GM_A', 'CV_GM_H', 'CV_GM_A',\n",
    " 'DifGM_H', 'Media_GS_H', 'Media_GS_A', 'DesvPad_GS_H', 'DesvPad_GS_A', 'CV_GS_H', 'CV_GS_A', 'DifGS_H', 'Media_SG_H', 'Media_SG_A',\n",
    " 'DesvPad_SG_H', 'DesvPad_SG_A', 'CV_SG_H', 'CV_SG_A', 'DifSG_H', 'Media_SG_H_01', 'Media_SG_A_01', 'DesvPad_SG_H_01', 'DesvPad_SG_A_01',\n",
    " 'CV_SG_H_01', 'CV_SG_A_01', 'Media_SG_H_02', 'Media_SG_A_02', 'DesvPad_SG_H_02', 'DesvPad_SG_A_02', 'CV_SG_H_02', 'CV_SG_A_02',\n",
    " 'Media_CG_H_02', 'Media_CG_A_02', 'DesvPad_CG_H_02', 'DesvPad_CG_A_02', 'CV_CG_H_02', 'CV_CG_A_02', 'Dif_CG_02', 'MediaOddsHome', 'MediaOddsAway',\n",
    " 'Media_Goals_Home', 'Media_Goals_Away', ODDS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "############ lembrar que adicionei o fix imbalance = true\n",
    "real_treino, real_teste = cf.setup_pycaret(data = treinamento, features = features, seed = 300, fix_imbalance=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top = pyc.compare_models(exclude=['catboost', 'dummy', 'lightgbm'], fold = 30, sort='precision', n_select=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pyc.create_model('nb', fold = 5, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filtra_resultados(_df, filter = 0):\n",
    "    _df = _df[_df['prediction_label'] == 1]\n",
    "\n",
    "    # if filter == 1:\n",
    "    #     _df = _df.query(\"2.791 < FT_Odds_D <= 3.288\")\n",
    "\n",
    "    return _df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdl = model\n",
    "flt = 1\n",
    "\n",
    "print('TREINO', '-'*80)\n",
    "df_treino = cf.info_model(mdl, real_treino, filtra_resultados, filter = flt)\n",
    "print('TESTE', '-'*80)\n",
    "df_teste = cf.info_model(mdl, real_teste, filtra_resultados, filter = flt)\n",
    "# print('TREINO & TESTE', '-'*80)\n",
    "# df_treinamento = cf.info_model(mdl, treinamento, filtra_resultados, filter = flt)\n",
    "print('VALIDAÇÃO', '-'*80)\n",
    "df_val = cf.info_model(mdl, val, filtra_resultados, filter = flt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Teste em todos os dfs de validação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(dfs)):\n",
    "    aux_dfs = cf.info_model(mdl, dfs[i], filtra_resultados, filter = flt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Completo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_treinamento_aux = cf.info_model(mdl, treinamento, filtra_resultados, filter = flt, show_info=False)\n",
    "df_val_aux = cf.info_model(mdl, entire_val, filtra_resultados, filter = flt, show_info=False)\n",
    "completo = pd.concat([df_treinamento_aux, df_val_aux])\n",
    "completo.reset_index(inplace=True, drop=True)\n",
    "\n",
    "print('COMPLETO ORDENADO POR DATA', '-'*80)\n",
    "df_df = cf.info_model(mdl, df, filtra_resultados, filter = flt)\n",
    "\n",
    "print('COMPLETO ORDENADO POR TREINO + VALIDAÇÃO', '-'*80)\n",
    "\n",
    "cf.plot_chart(completo, line_limit=len(df_treinamento_aux))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entire Val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_entire_val = cf.info_model(mdl, entire_val, filtra_resultados, filter = flt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Teste Val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teste_backup = real_teste.copy()\n",
    "teste_backup = teste_backup.merge(df[['Date', 'Home', 'Away', 'FTHG', 'FTAG']], how='left', left_index=True, right_index=True)\n",
    "teste_val = pd.concat([teste_backup, entire_val]).sort_values(by='Date').reset_index(drop=True)\n",
    "print('TESTE E VALIDAÇÃO', '-'*80)\n",
    "df_teste_val = cf.info_model(mdl, teste_val, filtra_resultados, filter = flt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Salvar modelo e histórico de validação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cf.export_val(df_teste_val)\n",
    "pyc.save_model(model, 'v2_lay_goals_away')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QCut automático"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "autocut_treino = df_treino.copy()\n",
    "autocut_teste = df_teste.copy()\n",
    "autocut_val = df_val.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_bins = 3\n",
    "max_bins = 15\n",
    "metrica = ['wr', 'pl']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_bins(train_data, test_data, val_data, features, bins, n_ranges, metric, limit, min_entries=0.02):\n",
    "    variaveis = features.copy()\n",
    "    ranges_dict = {}\n",
    "    filtered_ranges = []\n",
    "    string_list = []\n",
    "\n",
    "    for var in variaveis:\n",
    "        try:\n",
    "            new_dict = cf.find_top_range(var, n_ranges = n_ranges, treino = train_data, teste = test_data, validacao = val_data, bins = bins, target = 'Profit')\n",
    "            ranges_dict[var] = new_dict\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    ordenado = {chave: valor for chave, valor in sorted(ranges_dict.items(), key=lambda item: item[1]['metric_val'][metric[0]], reverse=True)}\n",
    "\n",
    "    for var in ordenado:\n",
    "        diff_treino = ordenado[var][\"metric_treino\"][metric[0]] - ordenado[var][\"metric_tr_base\"][metric[0]]\n",
    "        diff_teste = ordenado[var][\"metric_teste\"][metric[0]] - ordenado[var][\"metric_te_base\"][metric[0]]\n",
    "        diff_val = ordenado[var][\"metric_val\"][metric[0]] - ordenado[var][\"metric_val_base\"][metric[0]]\n",
    "        entradas = ordenado[var][\"metric_val\"][\"porc_ent\"]\n",
    "        if ((diff_treino > 0) & (diff_teste > 0) & (diff_val > 0) & (entradas > min_entries) & (abs(ordenado[var][\"metric_val\"][metric[0]] - ordenado[var][\"metric_teste\"][metric[0]]) < limit)):\n",
    "            string_list.append(f'{var} | {bins} bins | {n_ranges} ranges')\n",
    "\n",
    "            print(f'{var} | {bins} bins | {n_ranges} ranges')\n",
    "            print(f'treino: {ordenado[var][\"metric_treino\"][metric[0]]:.4f} ({ordenado[var][\"metric_treino\"][metric[0]] - ordenado[var][\"metric_tr_base\"][metric[0]]:.2f})   |   {ordenado[var][\"metric_treino\"][metric[1]]:.2f} ({ordenado[var][\"metric_treino\"][metric[1]] - ordenado[var][\"metric_tr_base\"][metric[1]]:.2f})')\n",
    "            print(f'teste: {ordenado[var][\"metric_teste\"][metric[0]]:.4f} ({ordenado[var][\"metric_teste\"][metric[0]] - ordenado[var][\"metric_te_base\"][metric[0]]:.2f})   |   {ordenado[var][\"metric_teste\"][metric[1]]:.2f} ({ordenado[var][\"metric_teste\"][metric[1]] - ordenado[var][\"metric_te_base\"][metric[1]]:.2f})')\n",
    "            print(f'val: {ordenado[var][\"metric_val\"][metric[0]]:.4f} ({ordenado[var][\"metric_val\"][metric[0]] - ordenado[var][\"metric_val_base\"][metric[0]]:.2f})   |   {ordenado[var][\"metric_val\"][metric[1]]:.2f} ({ordenado[var][\"metric_val\"][metric[1]] - ordenado[var][\"metric_val_base\"][metric[1]]:.2f})')\n",
    "            print(f'% Ent: {ordenado[var][\"metric_val\"][\"porc_ent\"]:.2f}')\n",
    "            ordenado[var]['variavel'] = var\n",
    "            filtered_ranges.append(ordenado[var])\n",
    "            print()\n",
    "\n",
    "    return filtered_ranges, string_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_dicts = []\n",
    "filters_string = ''\n",
    "\n",
    "for bin in range(max_bins - 1, min_bins - 1, -1):\n",
    "    for _ in range(bin-1, -1, -1):\n",
    "        ranges, string_list = test_bins(autocut_treino, autocut_teste, autocut_val, features, bins = bin, n_ranges = _ + 1, metric = metrica, limit = 0.01, min_entries=0.50)\n",
    "        if len(ranges) > 0:\n",
    "            for str_on_list in string_list:\n",
    "                filters_string += str_on_list + '\\n'\n",
    "            for i in range(len(ranges)):\n",
    "                intervalo = ranges[i]['range']\n",
    "                filtered_dicts.append({'bins': bin, 'n_ranges': _ + 1, 'range': intervalo, 'var': ranges[i]['variavel']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for bin in range(max_bins - 1, min_bins - 1, -1):\n",
    "#     for _ in range(bin-1, -1, -1):\n",
    "#         ranges = cf.test_bins(autocut_treino, autocut_teste, autocut_val, features, bins = bin, n_ranges = _ + 1, metric = metrica, limit = 0.02, min_entries=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_query(column, intervals):\n",
    "    query_parts = [f\"{interval.left} < {column} <= {interval.right}\" for interval in intervals]\n",
    "    query_string = \" or \".join(query_parts)\n",
    "    return query_string\n",
    "\n",
    "def create_filters_object(filters_string):\n",
    "    result = []\n",
    "    for line in filters_string.strip().split(\"\\n\"):\n",
    "        parts = line.split(\" | \")\n",
    "        result.append({\n",
    "            \"var\": parts[0],\n",
    "            \"bins\": int(parts[1].split()[0]),\n",
    "            \"ranges\": int(parts[2].split()[0])\n",
    "        })\n",
    "    return result\n",
    "\n",
    "def filter_by_string(dataframe, filter_intervals, use_model=True):\n",
    "    df = dataframe.copy()\n",
    "    if use_model:\n",
    "        df = df[df['prediction_label'] == 1]\n",
    "\n",
    "    for filter_interval in filter_intervals:\n",
    "        df = df.query(filter_interval)\n",
    "\n",
    "    return df\n",
    "\n",
    "def each_val_df(query_string, num_dfs=0, use_model=True):\n",
    "    wr_blocks = []\n",
    "\n",
    "    if num_dfs == 0:\n",
    "        num_dfs = len(dfs)\n",
    "\n",
    "    for dataframe in dfs[2:num_dfs]:\n",
    "        aux_dfs = pyc.predict_model(mdl, dataframe, verbose=False)\n",
    "        aux_dfs = filter_by_string(aux_dfs, query_string, use_model=use_model)\n",
    "        aux_dfs = cf.calculate_profit(aux_dfs)\n",
    "        metrics = cf.calculate_metrics(aux_dfs, total=dataframe.shape[0], return_metrics=True)\n",
    "        wr_blocks.append(metrics['wr'])\n",
    "\n",
    "    inf_lim = np.mean(wr_blocks) - 1 * np.std(wr_blocks)\n",
    "    sup_lim = np.mean(wr_blocks) + 1 * np.std(wr_blocks) if np.mean(wr_blocks) + 1 * np.std(wr_blocks) < 1 else 1\n",
    "\n",
    "    print(f'Média WR: {np.mean(wr_blocks) * 100 :.2f}')\n",
    "    print(f'Std: {np.std(wr_blocks) * 100 :.2f}')\n",
    "    print(f'Margem 1 Std: De {inf_lim * 100 :.2f}% a {sup_lim * 100 :.2f}%')\n",
    "\n",
    "    return inf_lim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "\n",
    "filters_from_string = create_filters_object(filters_string)\n",
    "best_inf_lim = -1000\n",
    "\n",
    "for f in filters_from_string:\n",
    "    try:\n",
    "        print(f['var'], f['bins'], f['ranges'])\n",
    "        found_range = pydash.find(filtered_dicts, lambda item: item['var'] == f['var'] and item['bins'] == f['bins'] and item['n_ranges'] == f['ranges'])['range']\n",
    "        q_str = create_query(f['var'], found_range)\n",
    "        inf_lim = each_val_df([\n",
    "            '-30.926 < Media_SG_H_01 <= -13.76 or 3.338 < Media_SG_H_01 <= 20.436',\n",
    "            '0.113 < DifPer_HomeAway <= 0.179 or 0.245 < DifPer_HomeAway <= 0.644',\n",
    "            '0.356 < H_A <= 0.617 or 0.813 < H_A <= 0.878',\n",
    "                q_str\n",
    "            ], num_dfs=10, use_model=True) # Define se o algoritmo vai usar as saídas do modelo ou não\n",
    "        if inf_lim >= best_inf_lim:\n",
    "            best_inf_lim = inf_lim\n",
    "            display(HTML('<p style=\"font-weight:bold; margin-top:-12px;\" >  ⬆️⬆️⬆️ NEW BEST ⬆️⬆️⬆️ </p>'))\n",
    "        print()\n",
    "    except:\n",
    "        print('erro\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_var = 'Media_Goals_Away'\n",
    "chosen_bins = 8\n",
    "chosen_n_ranges = 6\n",
    "\n",
    "query_str = create_query(chosen_var, pydash.find(filtered_dicts, lambda item: item['var'] == chosen_var and item['bins'] == chosen_bins and item['n_ranges'] == chosen_n_ranges)['range'])\n",
    "query_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filtra_resultados(_df, filtro = 0,):\n",
    "    _df = _df[_df['prediction_label'] == 1]\n",
    "\n",
    "    if filtro == 55:\n",
    "        _df = _df.query(\"-30.926 < Media_SG_H_01 <= -13.76 or 3.338 < Media_SG_H_01 <= 20.436\")\n",
    "        _df = _df.query(\"0.113 < DifPer_HomeAway <= 0.179 or 0.245 < DifPer_HomeAway <= 0.644\")\n",
    "        _df = _df.query(\"0.356 < H_A <= 0.617 or 0.813 < H_A <= 0.878\")\n",
    "        _df = _df.query(\"89.961 < Media_Goals_Away <= 94.85 or 99.7 < Media_Goals_Away <= 114.25 or 119.1 < Media_Goals_Away <= 128.8\")\n",
    "\n",
    "    return _df\n",
    "\n",
    "# print('TREINO', '-'*80)\n",
    "df_treino = cf.info_model(mdl, real_treino, filtra_resultados, filter = 55, show_info=True)\n",
    "# print('TESTE', '-'*80)\n",
    "df_teste = cf.info_model(mdl, real_teste, filtra_resultados, filter = 55, show_info=True)\n",
    "# print('TREINO & TESTE', '-'*80)\n",
    "# df_treinamento = cf.info_model(mdl, treinamento, filtra_resultados, filter = 55, show_info=True)\n",
    "# print('VALIDAÇÃO', '-'*80)\n",
    "df_val = cf.info_model(mdl, val, filtra_resultados, filter = 55, show_info=True)\n",
    "# df_entire_val = cf.info_model(mdl, entire_val, filtra_resultados, filter = 55, show_info=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "wr_blocks = []\n",
    "\n",
    "for i_df in dfs:\n",
    "    aux_dfs = pyc.predict_model(mdl, i_df, verbose=False)\n",
    "    aux_dfs = filtra_resultados(aux_dfs, filtro = 55)\n",
    "    aux_dfs = cf.calculate_profit(aux_dfs)\n",
    "    metrics = cf.calculate_metrics(aux_dfs, total=i_df.shape[0], return_metrics=True)\n",
    "    wr_blocks.append(metrics['wr'])\n",
    "\n",
    "print(f'Média WR: {np.mean(wr_blocks) * 100 :.2f}')\n",
    "print(f'Std: {np.std(wr_blocks) * 100 :.2f}')\n",
    "print(f'Margem 1 Std: De {(np.mean(wr_blocks) - np.std(wr_blocks)) * 100 :.2f}% a {(np.mean(wr_blocks) + np.std(wr_blocks)) * 100 :.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Média WR: 96.32\n",
    "Std: 2.68"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Teste em todos os dfs de validação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in dfs:\n",
    "    aux_dfs = cf.info_model(mdl, i, filtra_resultados, filter = 0)\n",
    "    aux_dfs = cf.info_model(mdl, i, filtra_resultados, filter = 55)\n",
    "    print('----------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Teste + Validação completa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teste_backup = real_teste.copy()\n",
    "teste_backup = teste_backup.merge(df[['Date', 'Home', 'Away', 'FTHG', 'FTAG']], how='left', left_index=True, right_index=True)\n",
    "teste_val = pd.concat([teste_backup, entire_val]).sort_values(by='Date').reset_index(drop=True)\n",
    "print('TESTE E VALIDAÇÃO', '-'*80)\n",
    "df_teste_val = cf.info_model(mdl, teste_val, filtra_resultados, filter = 55)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('VALIDAÇÃO TOTAL', '-'*80)\n",
    "df_entire_val = cf.info_model(mdl, entire_val, filtra_resultados, filter = 55)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('COMPLETO ORDENADO POR DATA', '-'*80)\n",
    "df_df = cf.info_model(mdl, clustered.query('Cluster == @CHOSEN_CLUSTER or Cluster == 2'), filtra_resultados, filter = 55)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QCut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cut = df_treino.copy()\n",
    "df_cut2 = df_teste.copy()\n",
    "df_cut3 = df_val.copy()\n",
    "bins = 10\n",
    "limiar = 100\n",
    "var_n = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variaveis = features.copy()\n",
    "for var in variaveis:\n",
    "    try:\n",
    "        cf.find_best_range(df_cut, df_cut2, df_cut3, var, TARGET, bins, limiar=limiar)\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cf.find_best_range(df_cut, df_cut2, df_cut3, 'Prob_1GA', TARGET, bins, limiar=limiar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filtra_resultados(_df, filtro = 0):\n",
    "    _df = _df[_df['prediction_label'] == 1]\n",
    "    if filtro == 55:\n",
    "        _df = _df.query(\"0.0299 < Prob_1GA <= 0.0898 or 0.15 < Prob_1GA <= 0.239\")\n",
    "\n",
    "    return _df\n",
    "\n",
    "print('TREINO', '-'*80)\n",
    "df_treino = cf.info_model(mdl, real_treino, filtra_resultados, filter = 55, show_info=True)\n",
    "print('TESTE', '-'*80)\n",
    "df_teste = cf.info_model(mdl, real_teste, filtra_resultados, filter = 55, show_info=True)\n",
    "print('TREINO & TESTE', '-'*80)\n",
    "df_treinamento = cf.info_model(mdl, treinamento, filtra_resultados, filter = 55, show_info=False)\n",
    "print('VALIDAÇÃO', '-'*80)\n",
    "df_val = cf.info_model(mdl, val, filtra_resultados, filter = 55, show_info=True)\n",
    "# print('COMPLETO', '-' *80)\n",
    "# completo = pd.concat([df_teste, df_val])\n",
    "# cf.calculate_metrics(completo, len(data))\n",
    "# cf.plot_chart(completo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "wr_blocks = []\n",
    "\n",
    "for i in range(len(dfs)):\n",
    "    aux_dfs = pyc.predict_model(mdl, dfs[i], verbose=False)\n",
    "    aux_dfs = filtra_resultados(aux_dfs, filtro = 55)\n",
    "    aux_dfs = cf.calculate_profit(aux_dfs)\n",
    "    metrics = cf.calculate_metrics(aux_dfs, total=df.shape[0], return_metrics=True)\n",
    "    wr_blocks.append(metrics['wr'])\n",
    "\n",
    "print(f'Média WR: {np.mean(wr_blocks) * 100 :.2f}')\n",
    "print(f'Std: {np.std(wr_blocks) * 100 :.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Teste em todos os dfs de validação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(dfs)):\n",
    "    aux_dfs = cf.info_model(mdl, dfs[i], filtra_resultados, filter = 55)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparação de modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, i_model in enumerate(top):\n",
    "    # try:\n",
    "        print(f'- {i_model.__class__.__name__} - {i}')\n",
    "        cf.info_model(i_model, real_teste, filtra_resultados, filter = 1)\n",
    "        # cf.info_model(i_model, teste_val, filtra_resultados, filter = 1)\n",
    "        # cf.info_model(i_model, val, filtra_resultados, filter = 1)\n",
    "        # cf.info_model(i_model, entire_val, filtra_resultados, filter = 55)\n",
    "    # except:\n",
    "    #     continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Análise de blends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blend_df = cf.info_model(top[1], teste_val, filtra_resultados, filter = 1)\n",
    "blend_df = cf.info_model(blended, teste_val, filtra_resultados, filter = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blended = pyc.blend_models([top[1], top[3]], fold = 5, verbose=False, optimize='precision')\n",
    "stacked = pyc.stack_models([top[1], top[3]], fold = 5, verbose=False, optimize='precision')\n",
    "\n",
    "blend_df = cf.info_model(blended, teste_val, filtra_resultados, filter = 1)\n",
    "blend_df = cf.info_model(stacked, teste_val, filtra_resultados, filter = 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
